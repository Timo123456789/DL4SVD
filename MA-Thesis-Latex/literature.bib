@article{Knoth2017,
   abstract = {1. In 2015, Sub-Saharan Africa witnessed a total of 61 violent conflicts with 20 of them being classified as highly violent conflicts, i.e. wars or limited wars (Heidelberg Institute for Internatio...)},
   author = {Christian Knoth and Edzer Pebesma},
   doi = {10.1080/01431161.2016.1266105},
   issn = {13665901},
   issue = {1},
   journal = {International Journal of Remote Sensing},
   month = {1},
   pages = {273-295},
   publisher = {Taylor & Francis},
   title = {Detecting dwelling destruction in Darfur through object-based change analysis of very high-resolution imagery},
   volume = {38},
   url = {https://www.tandfonline.com/doi/abs/10.1080/01431161.2016.1266105},
   year = {2017},
}
@article{Leblanc2014,
   abstract = {Airborne hyperspectral imaging (HSI) was assessed as a potential tool to locate single grave sites. While airborne HSI has shown to be useful to locate mass graves, it is expected the location of single graves would be an order of magnitude more difficult due to the smaller size and reduced mass of the targets. Two clearings were evaluated (through a blind test) as potential sites for containing at least one set of buried remains. At no time prior to submitting the locations of the potential burial sites from the HSI were the actual locations of the sites released or shared with anyone from the analysis team.The two HSI sensors onboard the aircraft span the range of 408-2524. nm. A range of indicators that exploit the narrow spectral and spatial resolutions of the two complimentary HSI sensors onboard the aircraft were calculated. Based on the co-occurrence of anomalous pixels within the expected range of the indicators three potential areas conforming to our underlying assumptions of the expected spectral responses (and spatial area) were determined. After submission of the predicted burial locations it was revealed that two of the targets were located within GPS error (10. m) of the true burial locations. Furthermore, due to the history of the TPOF site for burial work, investigation of the third target is being considered in the near future. The results clearly demonstrate promise for hyperspectral imaging to aid in the detection of buried remains, however further work is required before these results can justifiably be used in routine scenarios.},
   author = {G. Leblanc and M. Kalacska and R. Soffer},
   doi = {10.1016/J.FORSCIINT.2014.08.020},
   issn = {0379-0738},
   journal = {Forensic Science International},
   keywords = {Airborne remote sensing,Grave detection,Hyperspectral,Single burial},
   month = {12},
   pages = {17-23},
   pmid = {25447169},
   publisher = {Elsevier},
   title = {Detection of single graves by airborne hyperspectral imaging},
   volume = {245},
   year = {2014},
}
@article{Qian2014,
   abstract = {This study evaluates and compares the performance of four machine learning classifiers—support vector machine (SVM), normal Bayes (NB), classification and regression tree (CART) and K nearest neighbor (KNN)—to classify very high resolution images, using an object-based classification procedure. In particular, we investigated how tuning parameters affect the classification accuracy with different training sample sizes. We found that: (1) SVM and NB were superior to CART and KNN, and both could achieve high classification accuracy (>90%); (2) the setting of tuning parameters greatly affected classification accuracy, particularly for the most commonly-used SVM classifier; the optimal values of tuning parameters might vary slightly with the size of training samples;  (3) the size of training sample also greatly affected the classification accuracy, when the size of training sample was less than 125. Increasing the size of training samples generally led to the increase of classification accuracies for all four classifiers. In addition, NB and KNN were more sensitive to the sample sizes. This research provides insights into the selection of classifiers and the size of training samples. It also highlights the importance of the appropriate setting of tuning parameters for different machine learning classifiers and provides useful information for optimizing these parameters.},
   author = {Yuguo Qian and Weiqi Zhou and Jingli Yan and Weifeng Li and Lijian Han},
   doi = {10.3390/RS70100153},
   issn = {2072-4292},
   issue = {1},
   journal = {Remote Sensing 2015, Vol. 7, Pages 153-168},
   keywords = {based classification,machine learning classifiers,object,tuning parameters,urban area,very high resolution image},
   month = {12},
   pages = {153-168},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Comparing Machine Learning Classifiers for Object-Based Land Cover Classification Using Very High Resolution Imagery},
   volume = {7},
   url = {https://www.mdpi.com/2072-4292/7/1/153/htm https://www.mdpi.com/2072-4292/7/1/153},
   year = {2014},
}
@article{Kalacska2006,
   author = {M. Kalacska and L.S. Bell},
   doi = {10.1080/00085030.2006.10757132},
   issn = {0008-5030},
   issue = {1},
   journal = {Canadian Society of Forensic Science Journal},
   month = {1},
   pages = {1-13},
   title = {Remote Sensing as a Tool for the Detection of Clandestine Mass Graves},
   volume = {39},
   year = {2006},
}
@article{Hall2023,
   abstract = {The field of artificial intelligence is seeing the increased application of satellite imagery to analyse poverty in its various manifestations. This nascent but rapidly growing intersection of scholarship holds the potential to help us better understand poverty by leveraging big data and recent advances in machine vision. In this study, we statistically analyse the literature in the expanding field of welfare and poverty predictions from the combination of machine learning and satellite imagery. Here, we apply an integrative review method to extract key data on factors related to the predictive power of welfare. We found that the most important factors correlated to the predictive power of welfare are the number of pre-processing steps employed, the number of datasets used, the type of welfare indicator targeted and the choice of AI model. Studies that used stock measure indicators (assets) as targets achieved better performance—17 percentage points higher—in predicting welfare than those that targeted flow measures (income and consumption) ones. Additionally, we found that the combination of machine learning and deep learning significantly increases predictive power—by as much as 15 percentage points—compared to using either alone. Surprisingly, we found that the spatial resolution of the satellite imagery used is important but not critical to the performance as the relationship is positive but not statistically significant. These findings have important implications for future research in this domain and for anyone aspiring to use the methodology.},
   author = {Ola Hall and Francis Dompae and Ibrahim Wahab and Fred Mawunyo Dzanku},
   doi = {10.1002/JID.3751},
   issn = {1099-1328},
   issue = {7},
   journal = {Journal of International Development},
   keywords = {deep learning,machine learning,poverty analysis,satellite imagery,welfare},
   month = {10},
   pages = {1753-1768},
   publisher = {John Wiley & Sons, Ltd},
   title = {A review of machine learning and satellite imagery for poverty prediction: Implications for development research and applications},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/jid.3751 \\ https://onlinelibrary.wiley.com/doi/10.1002/jid.3751},
   year = {2023},
}
@misc{Kemper2011,
   abstract = {This paper presents a methodology for the detection of dwelling structures in Darfur camps to estimate the total number of dwellings per camp using GeoEye-1 satellite images. The method is based on a translation of the visual characterization of the searched structures into a morphological image processing chain. Two variants are described: the first variant extracts dwellings fully automatic for enumeration, while the second links the area covered by dwellings to visual interpretation results of representative samples for estimation of the total number of dwellings. Compared to the visual interpretation both produce similar results with correlation coefficients of 0.65 and 0.66 respectively leading to a mean error of 6% in the total number of dwellings. In complex camp settings, the area-based approach might be preferred, since it provides some more control due to the visual interpretation included. Index Terms-Geo-Eye, image analysis, image interpretation, informal settlements, mathematical morphology.},
   author = {Thomas Kemper and Malgorzata Jenerowicz and Martino Pesaresi and Pierre Soille},
   doi = {10.1109/JSTARS.2010.2053700},
   issue = {1},
   journal = {IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING},
   keywords = {Geo-Eye,image analysis,image interpretation,informal settlements,mathematical morphology},
   title = {Enumeration of Dwellings in Darfur Camps From GeoEye-1 Satellite Images Using Mathematical Morphology},
   volume = {4},
   url = {http://ieeexplore.ieee.org.},
   year = {2011},
}
@misc{Kemper2011,
   abstract = {This paper presents a methodology for the detection of dwelling structures in Darfur camps to estimate the total number of dwellings per camp using GeoEye-1 satellite images. The method is based on a translation of the visual characterization of the searched structures into a morphological image processing chain. Two variants are described: the first variant extracts dwellings fully automatic for enumeration, while the second links the area covered by dwellings to visual interpretation results of representative samples for estimation of the total number of dwellings. Compared to the visual interpretation both produce similar results with correlation coefficients of 0.65 and 0.66 respectively leading to a mean error of 6% in the total number of dwellings. In complex camp settings, the area-based approach might be preferred, since it provides some more control due to the visual interpretation included. Index Terms-Geo-Eye, image analysis, image interpretation, informal settlements, mathematical morphology.},
   author = {Thomas Kemper and Malgorzata Jenerowicz and Martino Pesaresi and Pierre Soille},
   doi = {10.1109/JSTARS.2010.2053700},
   issue = {1},
   journal = {IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING},
   keywords = {Geo-Eye,image analysis,image interpretation,informal settlements,mathematical morphology},
   title = {Enumeration of Dwellings in Darfur Camps From GeoEye-1 Satellite Images Using Mathematical Morphology},
   volume = {4},
   url = {http://ieeexplore.ieee.org.},
   year = {2011},
}

@thesis{Balzer2022, 
   author = {Josefina L. Balzer}, 
   title = {Deep Learning zur Objektdetektion auf hochauflösenden Satellitenbildern},
   school = {Westfälische Wilhelms-Universität Münster},
   type = {Bachelorarbeit},
   year = {2022},
   note = {Erstgutachter: Marlon Becker, Zweitgutachter: Prof. Dr. Benjamin Risse}
}

@article{Benz2004,
   abstract = {Remote sensing from airborne and spaceborne platforms provides valuable data for mapping, environmental monitoring, disaster management and civil and military intelligence. However, to explore the full value of these data, the appropriate information has to be extracted and presented in standard format to import it into geo-information systems and thus allow efficient decision processes. The object-oriented approach can contribute to powerful automatic and semi-automatic analysis for most remote sensing applications. Synergetic use to pixel-based or statistical signal processing methods explores the rich information contents. Here, we explain principal strategies of object-oriented analysis, discuss how the combination with fuzzy methods allows implementing expert knowledge and describe a representative example for the proposed workflow from remote sensing imagery to GIS. The strategies are demonstrated using the first object-oriented image analysis software on the market, eCognition, which provides an appropriate link between remote sensing imagery and GIS. © 2003 Elsevier B.V. All rights reserved.},
   author = {Ursula C. Benz and Peter Hofmann and Gregor Willhauck and Iris Lingenfelder and Markus Heynen},
   doi = {10.1016/J.ISPRSJPRS.2003.10.002},
   issn = {0924-2716},
   issue = {3-4},
   journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
   keywords = {Fuzzy classification,GIS,Multi-resolution segmentation,Object-oriented image analysis,Remote sensing},
   month = {1},
   pages = {239-258},
   publisher = {Elsevier},
   title = {Multi-resolution, object-oriented fuzzy analysis of remote sensing data for GIS-ready information},
   volume = {58},
   year = {2004},
}
@article{Kattenborn2019,
   abstract = {Recent technological advances in remote sensing sensors and platforms, such as high-resolution satellite imagers or unmanned aerial vehicles (UAV), facilitate the availability of fine-grained earth observation data. Such data reveal vegetation canopies in high spatial detail. Efficient methods are needed to fully harness this unpreceded source of information for vegetation mapping. Deep learning algorithms such as Convolutional Neural Networks (CNN) are currently paving new avenues in the field of image analysis and computer vision. Using multiple datasets, we test a CNN-based segmentation approach (U-net) in combination with training data directly derived from visual interpretation of UAV-based high-resolution RGB imagery for fine-grained mapping of vegetation species and communities. We demonstrate that this approach indeed accurately segments and maps vegetation species and communities (at least 84% accuracy). The fact that we only used RGB imagery suggests that plant identification at very high spatial resolutions is facilitated through spatial patterns rather than spectral information. Accordingly, the presented approach is compatible with low-cost UAV systems that are easy to operate and thus applicable to a wide range of users.},
   author = {Teja Kattenborn and Jana Eichel and Fabian Ewald Fassnacht},
   doi = {10.1038/s41598-019-53797-9},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   month = {12},
   pmid = {31776370},
   publisher = {Nature Research},
   title = {Convolutional Neural Networks enable efficient, accurate and fine-grained segmentation of plant species and communities from high-resolution UAV imagery},
   volume = {9},
   year = {2019},
}
@misc{Spiegel_article,
   title = {Corona in China: Satellitenbilder von Krematorien deuten auf deutlich mehr Tote hin - DER SPIEGEL},
   url = {https://www.spiegel.de/ausland/corona-satellitenbilder-chinesischer-krematorien-deuten-auf-deutlich-mehr-tote-hin-a-c4305852-d092-4e54-a210-c88b820d564c},
   year = {2023},
}


@misc{Yang2022,
      title={Drone Object Detection Using RGB/IR Fusion}, 
      author={Lizhi Yang and Ruhang Ma and Avideh Zakhor},
      year={2022},
      eprint={2201.03786},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.03786}, 
}


@article{Gudius2021,
   abstract = {Satellite imagery is changing the way we understand and predict economic activity in the world. Advancements in satellite hardware and low-cost rocket launches have enabled near-real-time, high-resolution images covering the entire Earth. It is too labour-intensive, time-consuming and expensive for human annotators to analyse petabytes of satellite imagery manually. Current computer vision research exploring this problem still lack accuracy and prediction speed, both significantly important metrics for latency-sensitive automatized industrial applications. Here we address both of these challenges by proposing a set of improvements to the object recognition model design, training and complexity regularisation, applicable to a range of neural networks. Furthermore, we propose a fully convolutional neural network (FCN) architecture optimised for accurate and accelerated object recognition in multispectral satellite imagery. We show that our FCN exceeds human-level performance with state-of-the-art 97.67% accuracy over multiple sensors, it is able to generalize across dispersed scenery and outperforms other proposed methods to date. Its computationally light architecture delivers a fivefold improvement in training time and a rapid prediction, essential to real-time applications. To illustrate practical model effectiveness, we analyse it in algorithmic trading environment. Additionally, we publish a proprietary annotated satellite imagery dataset for further development in this research field. Our findings can be readily implemented for other real-time applications too.},
   author = {Povilas Gudžius and Olga Kurasova and Vytenis Darulis and Ernestas Filatovas},
   doi = {10.1007/S00138-021-01209-2/FIGURES/10},
   issn = {14321769},
   issue = {4},
   journal = {Machine Vision and Applications},
   keywords = {Communications Engineering,Image Processing and Computer Vision,Networks,Pattern Recognition},
   month = {7},
   pages = {1-14},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Deep learning-based object recognition in multispectral satellite imagery for real-time applications},
   volume = {32},
   url = {https://link.springer.com/article/10.1007/s00138-021-01209-2},
   year = {2021}
}


@article{Razakarivony2015,
   abstract = {To cite this version: Sébastien Razakarivony, Frédéric Jurie. Vehicle Detection in Aerial Imagery : A small target detection benchmark. Abstract This paper introduces VEDAI: Vehicle Detection in Aerial Imagery a new database of aerial images provided as a tool to benchmark automatic target recognition algorithms in unconstrained environments. The vehicles contained in the database, in addition of being small, exhibit different variabil-ities such as multiple orientations, lighting/shadowing changes, specularities or occlusions. Furthermore, each image is available in several spectral bands and resolutions. A precise experimental protocol is also given, ensuring that the experimental results obtained by different people can be properly reproduce and compared. Finally, the paper also gives the performance of baseline algorithms on this dataset, for different settings of these algorithms, to illustrate the difficulties of the task and provide baseline comparisons.},
   author = {Sébastien Razakarivony and Frédéric Jurie and Sebastien Razakarivony and Frederic Jurie},
   journal = {Journal of Visual Communication and Image Representation},
   keywords = {Aerial imagery,Computer vision,Database,Detection,Infrared imagery,Low resolution images,Vehicles},
   title = {Vehicle Detection in Aerial Imagery : A small target detection benchmark},
   url = {https://hal.science/hal-01122605v2},
   year = {2015}
}

@misc{vedai_web,
   title = {Vehicle Detection in Aerial Imagery (VEDAI) : a benchmark},
   url = {https://downloads.greyc.fr/vedai/}
}


@article{Wiley2018,
   abstract = {Computer vision has been studied from many persective. It expands from raw data recording into techniques and ideas combining digital image processing, pattern recognition, machine learning and computer graphics. The wide usage has attracted many scholars to integrate with many disciplines and fields. This paper provide a survey of the recent technologies and theoretical concept explaining the development of computer vision especially related to image processing using different areas of their field application. Computer vision helps scholars to analyze images and video to obtain necessary information, understand information on events or descriptions, and scenic pattern. It used method of multi-range application domain with massive data analysis. This paper provides contribution of recent development on reviews related to computer vision, image processing, and their related studies. We categorized the computer vision mainstream into four group e.g., image processing, object recognition, and machine learning. We also provide brief explanation on the up-to-date information about the techniques and their performance.},
   author = {Victor Wiley and Thomas Lucas},
   doi = {10.29099/ijair.v2i1.42},
   issue = {1},
   journal = {International Journal of Artificial Intelligence Research},
   month = {6},
   pages = {22},
   publisher = {STMIK Dharma Wacana},
   title = {Computer Vision and Image Processing: A Paper Review},
   volume = {2},
   year = {2018}
}


@article{Mery2013,
   abstract = {noabstract},
   author = {Domingo Mery and Franco Pedreschi and Alvaro Soto},
   doi = {10.1007/s11947-012-0934-2},
   issn = {19355130},
   issue = {8},
   journal = {Food and Bioprocess Technology},
   keywords = {Classification,Computer vision,Feature extraction,Feature selection,Food quality evaluation,Image analysis,Image processing,Image segmentation,Pattern recognition},
   month = {8},
   pages = {2093-2108},
   title = {Automated Design of a Computer Vision System for Visual Food Quality Evaluation},
   volume = {6},
   year = {2013}
}


@article{Matiacevich2013,
   abstract = {noabstract},
   author = {Silvia Matiacevich and Daniela Celis Cofré and Patricia Silva and Javier Enrione and Fernando Osorio},
   doi = {10.1155/2013/419535},
   issn = {23145765},
   journal = {International Journal of Food Science},
   publisher = {Hindawi Publishing Corporation},
   title = {Quality parameters of six cultivars of blueberry using computer vision},
   volume = {2013},
   year = {2013}
}


@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Braga-Neto2020,
   abstract = {Fundamentals of Pattern Recognition and Machine Learning is designed for a one or two-semester introductory course in Pattern Recognition or Machine Learning at the graduate or advanced undergraduate level. The book combines theory and practice and is suitable to the classroom and self-study. It has grown out of lecture notes and assignments that the author has developed while teaching classes on this topic for the past 13 years at Texas A and M University. The book is intended to be concise but thorough. It does not attempt an encyclopedic approach, but covers in significant detail the tools commonly used in pattern recognition and machine learning, including classification, dimensionality reduction, regression, and clustering, as well as recent popular topics such as Gaussian process regression and convolutional neural networks. In addition, the selection of topics has a few features that are unique among comparable texts: it contains an extensive chapter on classifier error estimation, as well as sections on Bayesian classification, Bayesian error estimation, separate sampling, and rank-based classification. The book is mathematically rigorous and covers the classical theorems in the area. Nevertheless, an effort is made in the book to strike a balance between theory and practice. In particular, examples with datasets from applications in bioinformatics and materials informatics are used throughout to illustrate the theory. These datasets are available from the book website to be used in end-of-chapter coding assignments based on python and scikit-learn. All plots in the text were generated using python scripts, which are also available on the book website.},
   author = {Ulisses Braga-Neto},
   doi = {10.1007/978-3-030-27656-0/COVER},
   isbn = {9783030276560},
   journal = {Fundamentals of Pattern Recognition and Machine Learning},
   keywords = {Bootstrap,Classification,Clustering,Cross-Validation,Decision Trees,Error Estimation,Feature Selection,Gaussian Mixture Modeling,Gaussian Process,K-means Clustering,Linear Discriminant Analysis,Machine Learning,Multidimensional Scaling,Neural Networks,Pattern Recognition,Perceptron,Principal Component Analysis,Regression,Support Vector Machines,Vapnik-Chervonenkis Theory},
   month = {1},
   pages = {1-357},
   publisher = {Springer International Publishing},
   title = {Fundamentals of Pattern Recognition and Machine Learning},
   year = {2020}
}
@article{Shetty2022,
   abstract = {The fundamental goal of machine learning (ML) is to inculcate computers to use data or former practice to resolve a specified problem. Artificial intelligence has given us incredible web search, self-driving vehicles, practical speech affirmation, and a massively better cognizance of human genetic data. An exact range of effective programs of ML already exist, which comprises classifiers to swot e-mail messages to study that allows distinguishing between unsolicited mail and non-spam messages. ML can be implemented as class analysis over supervised, unsupervised, and reinforcement learning. Supervised ML (SML) is the subordinate branch of ML and habitually counts on a domain skilled expert who "teaches" the learning scheme with required supervision. It also generates a task that maps inputs to chosen outputs. SML is genuinely normal in characterization issues since the aim is to get the computer, familiar with created descriptive framework. The data annotation is termed as a training set and the testing set as unannotated data. When annotations are discrete in the value, they are called class labels and continuous numerical annotations as continuous target values. The objective of SML is to form a compact prototype of the distribution of class labels in terms of predictor types. The resultant classifier is then used to designate class labels to the testing sets where the estimations of the predictor types are known, yet the values of the class labels are unidentified. Under certain assumptions, the larger the size of the training set, the better the expectations on the test set. This motivates the requirement for numerous area specialists or even different non-specialists giving names to preparing the framework. SML problems are grouped into classification and regression. In Classification the result has discrete value and the aim is to predict the discrete values fitting to a specific class. Regression is acquired from the Labeled Datasets and continuous-valued result are predicted for the latest data which is given to the algorithm. When choosing an SML algorithm, the heterogeneity, precision, excess, and linearity of the information ought to be examined before selecting an algorithm. SML is used in a various range of applications such as speech and object recognition, bioinformatics, and spam detection. Recently, advances in SML are being witnessed in solid-state material science for calculating material properties and predicting their structure. This review covers various algorithms and real-world applications of SML. The key advantage of SML is that, once an algorithm swots with data, it can do its task automatically.},
   author = {Shruthi H. Shetty and Sumiksha Shetty and Chandra Singh and Ashwath Rao},
   doi = {10.1002/9781119821908.CH1},
   isbn = {9781119821908},
   journal = {Fundamentals and Methods of Machine and Deep Learning: Algorithms, Tools, and Applications},
   keywords = {Artificial intelligence,Decision tree,Deep learning,Linear regression,Logistic regression,SVM,Solid state material science,Supervised machine learning},
   month = {1},
   pages = {1-16},
   publisher = {wiley},
   title = {Supervised machine learning: Algorithms and applications},
   year = {2022}
}

@article{Fischer1999,
   author = {Paul Fischer},
   city = {Wiesbaden},
   doi = {10.1007/978-3-663-11956-2},
   isbn = {978-3-519-02946-5},
   publisher = {Vieweg+Teubner Verlag},
   title = {Algorithmisches Lernen},
   url = {http://link.springer.com/10.1007/978-3-663-11956-2},
   year = {1999}
}

@misc{palma_spec,
  title        = {HPC Documentation PALMA II Hardware},
  author       = {{Center for Information Technology, University of Münster}},
  url          = {https://palma.uni-muenster.de/documentation/},
  note         = {Accessed: 2025-07-30}
}

@misc{numpy_main_web,
mendeley-groups = {BA Thesis},
title = {{NumPy}},
url = {https://numpy.org/},
urldate = {2025-08-04}
}
@misc{numpy_about,
mendeley-groups = {BA Thesis},
title = {{NumPy - About Us}},
url = {https://numpy.org/about/},
urldate = {2025-08-04}
}

@misc{opencv_about,
mendeley-groups = {BA Thesis},
title = {{About - OpenCV}},
url = {https://opencv.org/about/},
urldate = {2025-08-04}
}

@misc{opencv_release,
mendeley-groups = {BA Thesis},
title = {{Releases - OpenCV}},
url = {https://opencv.org/releases/},
urldate = {2025-08-04}
}

@misc{scipy-main,
mendeley-groups = {BA Thesis},
title = {{SciPy - Main}},
url = {https://scipy.org/},
urldate = {2025-04-08}
}

@misc{matplotlib,
mendeley-groups = {BA Thesis},
title = {{matplotlib - Main}},
url = {https://matplotlib.org/},
urldate = {2025-08-04}
}

@misc{seaborn,
mendeley-groups = {BA Thesis},
title = {{seaborn - Main}},
url = {https://seaborn.pydata.org/},
urldate = {2025-08-04}
}

@misc{pandas,
mendeley-groups = {BA Thesis},
title = {{pandas - Main}},
url = {https://pandas.pydata.org/},
urldate = {2025-08-04}
}


@misc{planet_labs,
mendeley-groups = {BA Thesis},
title = {{Planet Labs - Main}},
url = {https://www.planet.com/products/high-resolution-satellite-imagery/},
urldate = {2025-08-22}
}

@misc{airbus_neo,
mendeley-groups = {BA Thesis},
title = {{Airbus - Pleiades Neo}},
url = {https://space-solutions.airbus.com/imagery/our-optical-and-radar-satellite-imagery/pleiades-neo/},
urldate = {2025-08-22}
}


@misc{yolo_v9u_github,
mendeley-groups = {BA Thesis},
title = {{YOLOv9u - Github Repository}},
author       = {{Wing Kin Yiu}},
url = {https://github.com/WongKinYiu/yolov9/tree/yolov9u?tab=readme-ov-file},
date = {2024-02-22},
urldate = {2025-08-04}
}

@article{wang2024,
  title={{YOLOv9}: Learning What You Want to Learn Using Programmable Gradient Information},
  author={Wang, Chien-Yao  and Liao, Hong-Yuan Mark},
  booktitle={arXiv preprint arXiv:2402.13616},
  year={2024}
}

@article{Rainio2024,
   abstract = {Research on different machine learning (ML) has become incredibly popular during the past few decades. However, for some researchers not familiar with statistics, it might be difficult to understand how to evaluate the performance of ML models and compare them with each other. Here, we introduce the most common evaluation metrics used for the typical supervised ML tasks including binary, multi-class, and multi-label classification, regression, image segmentation, object detection, and information retrieval. We explain how to choose a suitable statistical test for comparing models, how to obtain enough values of the metric for testing, and how to perform the test and interpret its results. We also present a few practical examples about comparing convolutional neural networks used to classify X-rays with different lung infections and detect cancer tumors in positron emission tomography images.},
   author = {Oona Rainio and Jarmo Teuho and Riku Klén},
   doi = {10.1038/s41598-024-56706-x},
   issn = {2045-2322},
   issue = {1},
   journal = {Scientific Reports},
   month = {3},
   pages = {6086},
   title = {Evaluation metrics and statistical tests for machine learning},
   volume = {14},
   year = {2024}
}

@article{Chen2014,
   author = {Yushi Chen and Zhouhan Lin and Xing Zhao and Gang Wang and Yanfeng Gu},
   doi = {10.1109/JSTARS.2014.2329330},
   issn = {1939-1404},
   issue = {6},
   journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
   month = {6},
   pages = {2094-2107},
   title = {Deep Learning-Based Classification of Hyperspectral Data},
   volume = {7},
   year = {2014}
}


@article{Zhu2017,
   author = {Xiao Xiang Zhu and Devis Tuia and Lichao Mou and Gui-Song Xia and Liangpei Zhang and Feng Xu and Friedrich Fraundorfer},
   doi = {10.1109/MGRS.2017.2762307},
   issn = {2168-6831},
   issue = {4},
   journal = {IEEE Geoscience and Remote Sensing Magazine},
   month = {12},
   pages = {8-36},
   title = {Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources},
   volume = {5},
   year = {2017}
}


@article{Kussul2017,
   author = {Nataliia Kussul and Mykola Lavreniuk and Sergii Skakun and Andrii Shelestov},
   doi = {10.1109/LGRS.2017.2681128},
   issn = {1545-598X},
   issue = {5},
   journal = {IEEE Geoscience and Remote Sensing Letters},
   month = {5},
   pages = {778-782},
   title = {Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data},
   volume = {14},
   year = {2017}
}

@article{Bhagavathy2006,
   author = {S. Bhagavathy and B.S. Manjunath},
   doi = {10.1109/TGRS.2006.881741},
   issn = {0196-2892},
   issue = {12},
   journal = {IEEE Transactions on Geoscience and Remote Sensing},
   month = {12},
   pages = {3706-3715},
   title = {Modeling and Detection of Geospatial Objects Using Texture Motifs},
   volume = {44},
   year = {2006}
}

@article{Cheng2016,
   author = {Gong Cheng and Peicheng Zhou and Junwei Han},
   doi = {10.1109/TGRS.2016.2601622},
   issn = {0196-2892},
   issue = {12},
   journal = {IEEE Transactions on Geoscience and Remote Sensing},
   month = {12},
   pages = {7405-7415},
   title = {Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images},
   volume = {54},
   year = {2016}
}

@article{Chen2006,
   author = {Xiao-Ling Chen and Hong-Mei Zhao and Ping-Xiang Li and Zhi-Yong Yin},
   doi = {10.1016/j.rse.2005.11.016},
   issn = {00344257},
   issue = {2},
   journal = {Remote Sensing of Environment},
   month = {9},
   pages = {133-146},
   title = {Remote sensing image-based analysis of the relationship between urban heat island and land use/cover changes},
   volume = {104},
   year = {2006}
}

@article{Eikvil2009,
   author = {Line Eikvil and Lars Aurdal and Hans Koren},
   doi = {10.1016/j.isprsjprs.2008.09.005},
   issn = {09242716},
   issue = {1},
   journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
   month = {1},
   pages = {65-72},
   title = {Classification-based vehicle detection in high-resolution satellite images},
   volume = {64},
   year = {2009}
}

@article{Wieland2023,
   author = {Marc Wieland and Sandro Martinis and Ralph Kiefl and Veronika Gstaiger},
   doi = {10.1016/j.rse.2023.113452},
   issn = {00344257},
   journal = {Remote Sensing of Environment},
   month = {3},
   pages = {113452},
   title = {Semantic segmentation of water bodies in very high-resolution satellite and aerial images},
   volume = {287},
   year = {2023}
}

@inbook{Chen2017,
   author = {Chenyi Chen and Ming-Yu Liu and Oncel Tuzel and Jianxiong Xiao},
   doi = {10.1007/978-3-319-54193-8_14},
   pages = {214-230},
   title = {R-CNN for Small Object Detection},
   year = {2017}
}


@article{XueyunChen2014,
   author = {Xueyun Chen and Shiming Xiang and Cheng-Lin Liu and Chun-Hong Pan},
   doi = {10.1109/LGRS.2014.2309695},
   issn = {1545-598X},
   issue = {10},
   journal = {IEEE Geoscience and Remote Sensing Letters},
   month = {10},
   pages = {1797-1801},
   title = {Vehicle Detection in Satellite Images by Hybrid Deep Convolutional Neural Networks},
   volume = {11},
   year = {2014}
}




@article{Jiang2015,
   abstract = {Vehicle detection in satellite images is an Challenging task, but meaningful at the same time. This paper propose a vehicle detection method in satellite images using Deep Convolutional Neural Network(DNN). DNN is a model of deep learning and it has a high learning capacity when dealing with images. DNN consist of several convolution layers and pooling layers, the last layer is full connection with output(this can be considered as neural network). DNN can automatically learn rich features from trainning dataset, and has achieved excellent performance in many applications such as image classification and object recognition. To benefit from this method, we propose a vehicle detection framework. Firstly we use a graph-based superpixel segmentation to extract a set of image patches, which can help us locate vehicle effectively. And then we train a DNN network to classify these pathes into vehicle and non-vehicle. Experimental results indicate that the proposed method has a good performance, with high detection rates and very few false alarms for all test road segment.},
   author = {Qiling Jiang and Liujuan Cao and Ming Cheng and Cheng Wang and Jonathan Li},
   doi = {10.1109/ISBB.2015.7344954},
   isbn = {9781467366090},
   journal = {4th International Symposium on Bioelectronics and Bioinformatics, ISBB 2015},
   month = {12},
   pages = {184-187},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Deep neural networks-based vehicle detection in satellite images},
   year = {2015}
}

@article{Zhou2016,
   author = {Peicheng Zhou and Gong Cheng and Zhenbao Liu and Shuhui Bu and Xintao Hu},
   doi = {10.1007/s11045-015-0370-3},
   issn = {0923-6082},
   issue = {4},
   journal = {Multidimensional Systems and Signal Processing},
   month = {10},
   pages = {925-944},
   title = {Weakly supervised target detection in remote sensing images based on transferred deep features and negative bootstrapping},
   volume = {27},
   year = {2016}
}


@article{Dalal2005,
   abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds. © 2005 IEEE.},
   author = {Navneet Dalal and Bill Triggs},
   doi = {10.1109/CVPR.2005.177},
   isbn = {0769523722},
   journal = {Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005},
   pages = {886-893},
   title = {Histograms of oriented gradients for human detection},
   volume = {I},
   year = {2005}
}


@article{Zhang2015,
   abstract = {Automatic oil tank detection plays a very important role for remote sensing image processing. To accomplish the task, a hierarchical oil tank detector with deep surrounding features is proposed in this paper. The surrounding features extracted by the deep learning model aim at making the oil tanks more easily to recognize, since the appearance of oil tanks is a circle and this information is not enough to separate targets from the complex background. The proposed method is divided into three modules: 1 candidate selection; 2 feature extraction; and 3 classification. First, a modified ellipse and line segment detector (ELSD) based on gradient orientation is used to select candidates in the image. Afterward, the feature combing local and surrounding information together is extracted to represent the target. Histogram of oriented gradients (HOG) which can reliably capture the shape information is extracted to characterize the local patch. For the surrounding area, the convolutional neural network (CNN) trained in ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) contest is applied as a blackbox feature extractor to extract rich surrounding feature. Then, the linear support vector machine (SVM) is utilized as the classifier to give the final output. Experimental results indicate that the proposed method is robust under different complex backgrounds and has high detection rate with low false alarm.},
   author = {Lu Zhang and Zhenwei Shi and Jun Wu},
   doi = {10.1109/JSTARS.2015.2467377},
   issn = {21511535},
   issue = {10},
   journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
   keywords = {Convolutional neural network (CNN),deep learning,ellipse and line segment detector (ELSD),oil tank detection,surrounding information},
   month = {10},
   pages = {4895-4909},
   publisher = {Institute of Electrical and Electronics Engineers},
   title = {A Hierarchical Oil Tank Detector with Deep Surrounding Features for High-Resolution Optical Satellite Imagery},
   volume = {8},
   year = {2015}
}

@article{Sevo2016,
   abstract = {},
   doi = {10.1109/LGRS.2016.2542358},
   issn = {1545598X},
   issue = {5},
   journal = {IEEE Geoscience and Remote Sensing Letters},
   keywords = {Aerial images,classification,convolutional neural networks (cNNs),object detection},
   month = {5},
   pages = {740-744},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Convolutional neural network based automatic object detection on aerial images},
   volume = {13},
   year = {2016}
}




@article{Zhu2015,
   abstract = {Detecting objects in aerial images is challenged by variance of object colors, aspect ratios, cluttered backgrounds, and in particular, undetermined orientations. In this paper, we propose to use Deep Convolutional Neural Network (DCNN) features from combined layers to perform orientation robust aerial object detection. We explore the inherent characteristics of DC-NN as well as relate the extracted features to the principle of disentangling feature learning. An image segmentation based approach is used to localize ROIs of various aspect ratios, and ROIs are further classified into positives or negatives using an SVM classifier trained on DCNN features. With experiments on two datasets collected from Google Earth, we demonstrate that the proposed aerial object detection approach is simple but effective.},
   author = {Haigang Zhu and Xiaogang Chen and Weiqun Dai and Kun Fu and Qixiang Ye and Jianbin Jiao},
   doi = {10.1109/ICIP.2015.7351502},
   isbn = {9781479983391},
   issn = {15224880},
   journal = {Proceedings - International Conference on Image Processing, ICIP},
   keywords = {Aerial Object Detection,Deep Convolutional Neural Network,Orientation Robust},
   month = {12},
   pages = {3735-3739},
   publisher = {IEEE Computer Society},
   title = {Orientation robust object detection in aerial images using deep convolutional neural network},
   volume = {2015-December},
   year = {2015}
}

@article{Liu2017,
   author = {Peng Liu and Kim-Kwang Raymond Choo and Lizhe Wang and Fang Huang},
   doi = {10.1007/s00500-016-2247-2},
   issn = {1432-7643},
   issue = {23},
   journal = {Soft Computing},
   month = {12},
   pages = {7053-7065},
   title = {SVM or deep learning? A comparative study on remote sensing image classification},
   volume = {21},
   year = {2017}
}


@article{Khan2018,
   author = {Muhammad Jaleed Khan and Hamid Saeed Khan and Adeel Yousaf and Khurram Khurshid and Asad Abbas},
   doi = {10.1109/ACCESS.2018.2812999},
   issn = {2169-3536},
   journal = {IEEE Access},
   pages = {14118-14129},
   title = {Modern Trends in Hyperspectral Image Analysis: A Review},
   volume = {6},
   year = {2018}
}
@inproceedings{Takumi2017,
   author = {Karasawa Takumi and Kohei Watanabe and Qishen Ha and Antonio Tejero-De-Pablos and Yoshitaka Ushiku and Tatsuya Harada},
   city = {New York, NY, USA},
   doi = {10.1145/3126686.3126727},
   isbn = {9781450354165},
   booktitle = {Proceedings of the on Thematic Workshops of ACM Multimedia 2017},
   month = {10},
   pages = {35-43},
   publisher = {ACM},
   title = {Multispectral Object Detection for Autonomous Vehicles},
   year = {2017}
}


@article{Alzubaidi2021,
   abstract = {In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achieving outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contributed several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most important aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL techniques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion.},
   author = {Laith Alzubaidi and Jinglan Zhang and Amjad J. Humaidi and Ayad Al-Dujaili and Ye Duan and Omran Al-Shamma and J. Santamaría and Mohammed A. Fadhel and Muthana Al-Amidie and Laith Farhan},
   doi = {10.1186/s40537-021-00444-8},
   issn = {2196-1115},
   issue = {1},
   journal = {Journal of Big Data},
   month = {3},
   pages = {53},
   title = {Review of deep learning: concepts, CNN architectures, challenges, applications, future directions},
   volume = {8},
   year = {2021}
}


@inproceedings{Soviany2018,
   author = {Petru Soviany and Radu Tudor Ionescu},
   doi = {10.1109/SYNASC.2018.00041},
   isbn = {978-1-7281-0625-0},
   booktitle = {2018 20th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)},
   month = {9},
   pages = {209-214},
   publisher = {IEEE},
   title = {Optimizing the Trade-Off between Single-Stage and Two-Stage Deep Object Detectors using Image Difficulty Prediction},
   year = {2018}
}


@misc{ren2016,
      title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
      author={Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
      year={2016},
      eprint={1506.01497},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.01497}, 
}

@misc{he2018,
      title={Mask R-CNN}, 
      author={Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick},
      year={2018},
      eprint={1703.06870},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1703.06870}, 
}

@misc{redmon2016,
      title={You Only Look Once: Unified, Real-Time Object Detection}, 
      author={Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
      year={2016},
      eprint={1506.02640},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.02640}, 
}

@inbook{Liu_2016,
   title={SSD: Single Shot MultiBox Detector},
   ISBN={9783319464480},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-319-46448-0_2},
   DOI={10.1007/978-3-319-46448-0_2},
   booktitle={Computer Vision – ECCV 2016},
   publisher={Springer International Publishing},
   author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
   year={2016},
   pages={21–37} }

   @article{Sapkota2025,
   abstract = {This review systematically examines the progression of the You Only Look Once (YOLO) object detection algorithms from YOLOv1 to the recently unveiled YOLOv12. Employing a reverse chronological analysis, this study examines the advancements introduced by YOLO algorithms, beginning with YOLOv12 and progressing through YOLO11 (or YOLOv11), YOLOv10, YOLOv9, YOLOv8, and subsequent versions to explore each version’s contributions to enhancing speed, detection accuracy, and computational efficiency in real-time object detection. Additionally, this study reviews the alternative versions derived from YOLO architectural advancements of YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and Gold-YOLO. Moreover, the study highlights the transformative impact of YOLO models across five critical application areas: autonomous vehicles and traffic safety, healthcare and medical imaging, industrial manufacturing, surveillance and security, and agriculture. By detailing the incremental technological advancements in subsequent YOLO versions, this review chronicles the evolution of YOLO, and discusses the challenges and limitations in each of the earlier versions. The evolution signifies a path towards integrating YOLO with multimodal, context-aware, and Artificial General Intelligence (AGI) systems for the next YOLO decade, promising significant implications for future developments in AI-driven applications.},
   author = {Ranjan Sapkota and Marco Flores-Calero and Rizwan Qureshi and Chetan Badgujar and Upesh Nepal and Alwin Poulose and Peter Zeno and Uday Bhanu Prakash Vaddevolu and Sheheryar Khan and Maged Shoman and Hong Yan and Manoj Karkee},
   doi = {10.1007/s10462-025-11253-3},
   issn = {15737462},
   issue = {9},
   journal = {Artificial Intelligence Review},
   keywords = {Agriculture,Artificial intelligence,Autonomous vehicles,CNN,Computer vision,Deep learning,Healthcare and medical imaging,Industrial manufacturing,Real-time object detection,Surveillance,Traffic safety,YOLO,YOLO configurations,YOLOv1 to YOLOv12,You Only Look Once},
   month = {9},
   publisher = {Springer Nature},
   title = {YOLO advances to its genesis: a decadal and comprehensive review of the You Only Look Once (YOLO) series},
   volume = {58},
   year = {2025}
}


@inproceedings{Li2018,
   author = {Rui Li and Jun Yang},
   doi = {10.1109/ICMCS.2018.8525895},
   isbn = {978-1-5386-6220-5},
   booktitle = {2018 6th International Conference on Multimedia Computing and Systems (ICMCS)},
   month = {5},
   pages = {1-6},
   publisher = {IEEE},
   title = {Improved YOLOv2 Object Detection Model},
   year = {2018}
}

@inproceedings{Nakahara2018,
   author = {Hiroki Nakahara and Haruyoshi Yonekawa and Tomoya Fujii and Shimpei Sato},
   city = {New York, NY, USA},
   doi = {10.1145/3174243.3174266},
   isbn = {9781450356145},
   booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
   month = {2},
   pages = {31-40},
   publisher = {ACM},
   title = {A Lightweight YOLOv2},
   year = {2018}
}


@inproceedings{Kim2018,
   author = {Kwang-Ju Kim and Pyong-Kun Kim and Yun-Su Chung and Doo-Hyun Choi},
   doi = {10.1109/AVSS.2018.8639438},
   isbn = {978-1-5386-9294-3},
   booktitle = {2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)},
   month = {11},
   pages = {1-6},
   publisher = {IEEE},
   title = {Performance Enhancement of YOLOv3 by Adding Prediction Layers with Spatial Pyramid Pooling for Vehicle Detection},
   year = {2018}
}


@article{Nepal2022,
   abstract = { },
   author = {Upesh Nepal and Hossein Eslamiat},
   doi = {10.3390/s22020464},
   issn = {1424-8220},
   issue = {2},
   journal = {Sensors},
   month = {1},
   pages = {464},
   title = {Comparing YOLOv3, YOLOv4 and YOLOv5 for Autonomous Landing Spot Detection in Faulty UAVs},
   volume = {22},
   year = {2022}
}



@inbook{Mohod2023,
   author = {Nikita Mohod and Prateek Agrawal and Vishu Madaan},
   doi = {10.1007/978-3-031-28183-9_46},
   pages = {654-665},
   title = {YOLOv4 Vs YOLOv5: Object Detection on Surveillance Videos},
   year = {2023}
}


@article{Sozzi2022,
   abstract = { },
   author = {Marco Sozzi and Silvia Cantalamessa and Alessia Cogato and Ahmed Kayad and Francesco Marinello},
   doi = {10.3390/agronomy12020319},
   issn = {2073-4395},
   issue = {2},
   journal = {Agronomy},
   month = {1},
   pages = {319},
   title = {Automatic Bunch Detection in White Grape Varieties Using YOLOv3, YOLOv4, and YOLOv5 Deep Learning Algorithms},
   volume = {12},
   year = {2022}
}


@misc{ultralyics_2020,
mendeley-groups = {BA Thesis},
title = {{Ultralytics Github}},
author = {{Ultralytics}},
url = {https://github.com/ultralytics},
year = {2020},
urldate = {2025-08-26}
}




@misc{ultralyics_yolov9,
mendeley-groups = {BA Thesis},
title = {{Ultralytics Github}},
author = {{n. A.}},
url = {https://docs.ultralytics.com/models/yolov9/#supported-tasks-and-modes},
urldate = {2025-09-17}
}


@misc{ultralyics_2023,
mendeley-groups = {BA Thesis},
title = {{Ultralytics Github}},
author = {{G. Jocher and A. Chaurasia and J. Qiu}},
url = {https://github.com/ultralytics},
year = {2021},
urldate = {2025-08-26}
}


@misc{abc_utah,
mendeley-groups = {BA Thesis},
title = {{These are the most popular car colors in every state}},
author = {{Nextstar Media Wire and iSeeCars}},
url = {https://www.abc4.com/news/national/these-are-the-most-popular-car-colors-in-every-state/},
year = {2023},
urldate = {2025-09-18}
}



@misc{Airbus_Ship_Det,
mendeley-groups = {BA Thesis},
title = {{Airbus Ship Detection Challenge}},
author = {{Airbus}},
year = {2018},
url = {https://www.kaggle.com/competitions/airbus-ship-detection/data},
urldate = {2025-09-23}
}



https://github.com/Timo123456789/master_thesis


@misc{Github_timo,
mendeley-groups = {BA Thesis},
title = {{Master Thesis - Deep Learning for Small Vehicle Detection and Classification
on High-Resolution Multispectral Remote Sensing Imagery}},
author = {{Timo Lietmeyer}},
url = {https://github.com/Timo123456789/master_thesis},
year = {2025},
urldate = {2025-09-23}
}

@misc{bfs_Strahlung,
mendeley-groups = {BA Thesis},
title = {{Was versteht man unter Licht?}},
author = {{Bundesamt fuer Strahlenschutz}},
url = {https://www.bfs.de/DE/themen/opt/sichtbares-licht/einfuehrung/einfuehrung.html},
urldate = {2025-09-18}
}

@article{Wang2024_yolo_review,
   abstract = {This is a comprehensive review of the YOLO series of systems. Different from previous literature surveys, this review article re-examines the characteristics of the YOLO series from the latest technical point of view. At the same time, we also analyzed how the YOLO series continued to influence and promote real-time computer vision-related research and led to the subsequent development of computer vision and language models.We take a closer look at how the methods proposed by the YOLO series in the past ten years have affected the development of subsequent technologies and show the applications of YOLO in various fields. We hope this article can play a good guiding role in subsequent real-time computer vision development.},
   author = {Chien-Yao Wang and Hong-Yuan Mark Liao},
   month = {8},
   title = {YOLOv1 to YOLOv10: The fastest and most accurate real-time object detection systems},
   url = {http://arxiv.org/abs/2408.09332},
   year = {2024}
}



@misc{li2022,
      title={YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications}, 
      author={Chuyi Li and Lulu Li and Hongliang Jiang and Kaiheng Weng and Yifei Geng and Liang Li and Zaidan Ke and Qingyuan Li and Meng Cheng and Weiqiang Nie and Yiduo Li and Bo Zhang and Yufei Liang and Linyuan Zhou and Xiaoming Xu and Xiangxiang Chu and Xiaoming Wei and Xiaolin Wei},
      year={2022},
      eprint={2209.02976},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2209.02976}, 
}

@misc{wang2022,
      title={YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors}, 
      author={Chien-Yao Wang and Alexey Bochkovskiy and Hong-Yuan Mark Liao},
      year={2022},
      eprint={2207.02696},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2207.02696}, 
}

@inproceedings{Wang2023,
   author = {Chien-Yao Wang and Alexey Bochkovskiy and Hong-Yuan Mark Liao},
   doi = {10.1109/CVPR52729.2023.00721},
   isbn = {979-8-3503-0129-8},
   booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   pages = {7464-7475},
   publisher = {IEEE},
   title = {YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors},
   year = {2023}
}


@misc{wang2024_sapkota,
      title={YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information}, 
      author={Chien-Yao Wang and I-Hau Yeh and Hong-Yuan Mark Liao},
      year={2024},
      eprint={2402.13616},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.13616}, 
}

@misc{wang2024,
      title={YOLOv10: Real-Time End-to-End Object Detection}, 
      author={Ao Wang and Hui Chen and Lihao Liu and Kai Chen and Zijia Lin and Jungong Han and Guiguang Ding},
      year={2024},
      eprint={2405.14458},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.14458}, 
}

@misc{ultralyics_yolov11,
mendeley-groups = {BA Thesis},
title = {{Ultralytics}},
author = {{Ultralytics}},
url = {https://docs.ultralytics.com/de/models/yolo11/},
year = {2024},
urldate = {2025-08-26}
}

@misc{tian2025,
      title={YOLOv12: Attention-Centric Real-Time Object Detectors}, 
      author={Yunjie Tian and Qixiang Ye and David Doermann},
      year={2025},
      eprint={2502.12524},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.12524}, 
}

@book{Anguita2012,
   abstract = {The K-fold Cross Validation (KCV) technique is one of the most used approaches by practitioners for model selection and error estimation of classifiers. The KCV consists in splitting a dataset into k subsets; then, iteratively, some of them are used to learn the model, while the others are exploited to assess its performance. However, in spite of the KCV success, only practical rule-of-thumb methods exist to choose the number and the cardinality of the subsets. We propose here an approach , which allows to tune the number of the subsets of the KCV in a data-dependent way, so to obtain a reliable, tight and rigorous estimation of the probability of misclassification of the chosen model.},
   author = {Davide Anguita and Luca Ghelardoni and Alessandro Ghio and Luca Oneto and Sandro Ridella},
   isbn = {9782874190490},
   publisher = {European Symposium on Artificial Neural Networks, Computational  Intelligence  and Machine Learning},
   title = {The 'K' in K-fold Cross Validation},
   url = {http://www.i6doc.com/en/livre/?GCOI=28001100967420.},
   year = {2012}
}


@article{Nti2021,
   abstract = {The numerical value of k in a k-fold cross-validation training technique of machine learning predictive models is an essential element that impacts the model’s performance. A right choice of k results in better accuracy, while a poorly chosen value for k might affect the model’s performance. In literature, the most commonly used values of k are five (5) or ten (10), as these two values are believed to give test error rate estimates that suffer neither from extremely high bias nor very high variance. However, there is no formal rule. To the best of our knowledge, few experimental studies attempted to investigate the effect of diverse k values in training different machine learning models. This paper empirically analyses the prevalence and effect of distinct k values (3, 5, 7, 10, 15 and 20) on the validation performance of four well-known machine learning algorithms (Gradient Boosting Machine (GBM), Logistic Regression (LR), Decision Tree (DT) and K-Nearest Neighbours (KNN)). It was observed that the value of k and model validation performance differ from one machine-learning algorithm to another for the same classification task. However, our empirical suggest that k = 7 offers a slight increase in validations accuracy and area under the curve measure with lesser computational complexity than k = 10 across most MLA. We discuss in detail the study outcomes and outline some guidelines for beginners in the machine learning field in selecting the best k value and machine learning algorithm for a given task.},
   author = {Isaac Kofi Nti and Owusu Nyarko-Boateng and Justice Aning},
   doi = {10.5815/ijitcs.2021.06.05},
   issn = {20749007},
   issue = {6},
   journal = {International Journal of Information Technology and Computer Science},
   month = {12},
   pages = {61-71},
   title = {Performance of Machine Learning Algorithms with Different K Values in K-fold CrossValidation},
   volume = {13},
   year = {2021}
}


@misc{Rezatofighi2019,
      title={Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression}, 
      author={Hamid Rezatofighi and Nathan Tsoi and JunYoung Gwak and Amir Sadeghian and Ian Reid and Silvio Savarese},
      year={2019},
      eprint={1902.09630},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1902.09630}, 
}

@article{Ramirez2019,
   abstract = {Recent works have proven that many relevant visual tasks are closely related one to another. Yet, this connection is seldom deployed in practice due to the lack of practical methodologies to transfer learned concepts across different training processes. In this work, we introduce a novel adaptation framework that can operate across both task and domains. Our framework learns to transfer knowledge across tasks in a fully supervised domain (e.g., synthetic data) and use this knowledge on a different domain where we have only partial supervision (e.g., real data). Our proposal is complementary to existing domain adaptation techniques and extends them to cross tasks scenarios providing additional performance gains. We prove the effectiveness of our framework across two challenging tasks (i.e., monocular depth estimation and semantic segmentation) and four different domains (Synthia, Carla, Kitti, and Cityscapes).},
   author = {Pierluigi Zama Ramirez and Alessio Tonioni and Samuele Salti and Luigi Di Stefano},
   month = {10},
   title = {Learning Across Tasks and Domains},
   url = {http://arxiv.org/abs/1904.04744},
   year = {2019}
}


@misc{cordts2016,
      title={The Cityscapes Dataset for Semantic Urban Scene Understanding}, 
      author={Marius Cordts and Mohamed Omran and Sebastian Ramos and Timo Rehfeld and Markus Enzweiler and Rodrigo Benenson and Uwe Franke and Stefan Roth and Bernt Schiele},
      year={2016},
      eprint={1604.01685},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1604.01685}, 
}

@inproceedings{Zhou2017,
   author = {Bolei Zhou and Hang Zhao and Xavier Puig and Sanja Fidler and Adela Barriuso and Antonio Torralba},
   doi = {10.1109/CVPR.2017.544},
   isbn = {978-1-5386-0457-1},
   booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {7},
   pages = {5122-5130},
   publisher = {IEEE},
   title = {Scene Parsing through ADE20K Dataset},
   year = {2017}
}

@misc{lin2015,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
      year={2015},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1405.0312}, 
}


@article{Everingham2010,
   author = {Mark Everingham and Luc Van Gool and Christopher K. I. Williams and John Winn and Andrew Zisserman},
   doi = {10.1007/s11263-009-0275-4},
   issn = {0920-5691},
   issue = {2},
   journal = {International Journal of Computer Vision},
   month = {6},
   pages = {303-338},
   title = {The Pascal Visual Object Classes (VOC) Challenge},
   volume = {88},
   year = {2010}
}


@inbook{Kristan2016,
   author = {Matej Kristan and Aleš Leonardis and Jiři Matas and Michael Felsberg and Roman Pflugfelder and Luka Čehovin and Tomáš Vojír̃ and Gustav Häger and Alan Lukežič and Gustavo Fernández and Abhinav Gupta and Alfredo Petrosino and Alireza Memarmoghadam and Alvaro Garcia-Martin and Andrés Solís Montero and Andrea Vedaldi and Andreas Robinson and Andy J. Ma and Anton Varfolomieiev and Aydin Alatan and Aykut Erdem and Bernard Ghanem and Bin Liu and Bohyung Han and Brais Martinez and Chang-Ming Chang and Changsheng Xu and Chong Sun and Daijin Kim and Dapeng Chen and Dawei Du and Deepak Mishra and Dit-Yan Yeung and Erhan Gundogdu and Erkut Erdem and Fahad Khan and Fatih Porikli and Fei Zhao and Filiz Bunyak and Francesco Battistone and Gao Zhu and Giorgio Roffo and Gorthi R. K. Sai Subrahmanyam and Guilherme Bastos and Guna Seetharaman and Henry Medeiros and Hongdong Li and Honggang Qi and Horst Bischof and Horst Possegger and Huchuan Lu and Hyemin Lee and Hyeonseob Nam and Hyung Jin Chang and Isabela Drummond and Jack Valmadre and Jae-chan Jeong and Jae-il Cho and Jae-Yeong Lee and Jianke Zhu and Jiayi Feng and Jin Gao and Jin Young Choi and Jingjing Xiao and Ji-Wan Kim and Jiyeoup Jeong and João F. Henriques and Jochen Lang and Jongwon Choi and Jose M. Martinez and Junliang Xing and Junyu Gao and Kannappan Palaniappan and Karel Lebeda and Ke Gao and Krystian Mikolajczyk and Lei Qin and Lijun Wang and Longyin Wen and Luca Bertinetto and Madan Kumar Rapuru and Mahdieh Poostchi and Mario Maresca and Martin Danelljan and Matthias Mueller and Mengdan Zhang and Michael Arens and Michel Valstar and Ming Tang and Mooyeol Baek and Muhammad Haris Khan and Naiyan Wang and Nana Fan and Noor Al-Shakarji and Ondrej Miksik and Osman Akin and Payman Moallem and Pedro Senna and Philip H. S. Torr and Pong C. Yuen and Qingming Huang and Rafael Martin-Nieto and Rengarajan Pelapur and Richard Bowden and Robert Laganière and Rustam Stolkin and Ryan Walsh and Sebastian B. Krah and Shengkun Li and Shengping Zhang and Shizeng Yao and Simon Hadfield and Simone Melzi and Siwei Lyu and Siyi Li and Stefan Becker and Stuart Golodetz and Sumithra Kakanuru and Sunglok Choi and Tao Hu and Thomas Mauthner and Tianzhu Zhang and Tony Pridmore and Vincenzo Santopietro and Weiming Hu and Wenbo Li and Wolfgang Hübner and Xiangyuan Lan and Xiaomeng Wang and Xin Li and Yang Li and Yiannis Demiris and Yifan Wang and Yuankai Qi and Zejian Yuan and Zexiong Cai and Zhan Xu and Zhenyu He and Zhizhen Chi},
   doi = {10.1007/978-3-319-48881-3_54},
   pages = {777-823},
   title = {The Visual Object Tracking VOT2016 Challenge Results},
   year = {2016}
}
@misc{lealtaixé2015,
      title={MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking}, 
      author={Laura Leal-Taixé and Anton Milan and Ian Reid and Stefan Roth and Konrad Schindler},
      year={2015},
      eprint={1504.01942},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1504.01942}, 
}


@misc{iou_pic,
mendeley-groups = {BA Thesis},
title={Intersection over Union (IoU) for object detection}, 
author={Adrian Rosebrock},
url = {https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/},
date = {2016-11-06},
urldate = {2025-08-26}
}


@misc{ultralyics_iou,
mendeley-groups = {BA Thesis},
title={Intersection over Union (IoU)}, 
author={Ultralytics},
url = {https://www.ultralytics.com/glossary/intersection-over-union-iou},
urldate = {2025-08-26}
}


@misc{palma_uv,
mendeley-groups = {BA Thesis},
title={UV-An extremely fast Python package and project manager, written in Rust.}, 
author = {Astral Software Inc.},
url = {https://docs.astral.sh/uv/},
urldate = {2025-08-26}
}

@misc{overfitting_pic,
mendeley-groups = {BA Thesis},
title={018 PyTorch - Popular techniques to prevent the Overfitting in a Neural Networks}, 
author = {Strahinja Zivkovic},
date = {2021-11-08},
url = {https://datahacker.rs/018-pytorch-popular-techniques-to-prevent-the-overfitting-in-a-neural-networks/},
urldate = {2025-08-27}
}
