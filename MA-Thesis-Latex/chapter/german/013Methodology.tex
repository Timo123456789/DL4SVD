%!TEX root = ../thesis.tex
\chapter{Methodology}
\todo{Ablaufdiagramm einfügen}
\label{ch:methodology}
\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/Ablaufdiagramm_white}
    \caption{Flowchart for workflow. Source: own representation}
    \label{fig:Flowchart}
\end{figure}
Die methodische Vorgehensweise dieser Arbeit ist im zugehörigen Flowchart dargestellt\todo{Referenz zum Flowchart einfügen}. Als Datengrundlage dient das \acrshort{VEDAI}-Datenset, auf dessen Basis zunächst eine Entscheidung zwischen axis-aligned und oriented Bounding Boxes getroffen wird. Da aktuelle Versionen des \acrshort{YOLO}-Frameworks grundsätzlich mit beiden Formaten umgehen können, erfolgt die Auswahl anhand ihrer jeweiligen Eignung für die Aufgaben der Objekterkennung und -lokalisierung.

Im Anschluss wird der \Acrfull{DOTA} genutzt, um ein vortrainiertes Modell zu erzeugen, das als Ausgangspunkt für das Training mit Kanalpermutationen dient. Abhängig von der zuvor gewählten Bounding-Box-Variante wird auch die Channel Permutation mit diesem Format durchgeführt.

Im Rahmen der Experimente werden verschiedene Kanalpermutationen (s. Tab. \ref{tab:perm}) getestet.

\begin{table}[h!]
\centering
\begin{tabular}{l|c|c|c|c|c}
Permutation & Red (R) & Green (G) & Blue (B) & Infrared (IR) & NDVI \\
\hline
\acrshort{RGBIR}    & x & x & x & x &   \\
\acrshort{IRGB}     &   & x & x & x &   \\
\acrshort{RIRB}     & x &   & x & x &   \\
\acrshort{RGB}      & x & x & x &   &   \\
\acrshort{RGIR}     & x & x &   & x &   \\
\acrshort{RGBNDVI}  & x & x & x &   & x \\
\acrshort{GBNDVI}   &   & x & x &   & x \\
\end{tabular}
\caption{Channel permutations represented by included channels (x = channel present).}
\label{tab:perm}
\end{table}
Die Evaluation der Ergebnisse erfolgt auf Basis der \acrshort{mAP}$_{50\text{-}95}$-Werte, welche eine standardisierte Bewertung der Modellleistung über verschiedene \Acrshort{IoU}-Schwellen hinweg ermöglichen. 

Zuletzt wurde eine Ablationsstudie unter Verwendung der Kanäle \Acrfull{R}, \Acrfull{G}, \Acrfull{B}, \acrshort{IR} und \acrshort{NDVI} durchgeführt.

% \begin{itemize}
%     \item Im Flowchart (\todo{Ref einfügen}) ist die Methodik dieser Thesis zu erkennen
%     \item VEDAI Dataset als Grundlage; erste Entscheidung zwischen Axis Aligned und ORiented Bounding Boxen (YOLO kann mit beiden Formaten je nach VErsion umgehen, schauen welche Form besser geeignet für Objekterkennung und -detektierung ist)
%     \item dann Training des DOTA Datensatzes als Pretrained Model für die Channel Permutation
%     \item Je nach Bounding Box Evaluationsergebnis wird dann die Channel Permutation mit dieser BB Art durchgeführt. Folgende Permutationen werden berechnet: RGBIR; IRGB; RIRB; RGB, RGIR und sowie als Erweiterung optional RGBNDVI, GBNDVI
%     \item Anhand der Map50-95 Werte kann dann die Evaluation erfolgen
% \end{itemize}



\section{Preprocessing of input data}
Bei der Verarbeitung des VEDAI-Datensatzes traten mehrere technische Herausforderungen auf. 
Zunächst mussten die vorhandenen Label in das \textit{YOLOv9}-OBB-Format konvertiert werden, um mit dem gewählten Modell kompatibel zu sein. 
Dabei zeigte sich, dass ein kleiner Teil der Daten (7 von 3\,757 Instanzen) Koordinatenwerte aufwies, die außerhalb des zulässigen Bereichs lagen (d.\,h. kleiner als 0 oder größer als 1). 
Dieses Problem wurde durch den Einsatz von Exception Handling sowie durch Rundung der Werte auf den gültigen Wertebereich behoben. \todo{Beispielbilder für Randlage einfügen}

\section{YOLOv9}
Um Objekte auf hochauflösenden Satellitenbildern zu detektieren, bietet sich der \Acrfull{YOLO} Algorithmus an. Dieser Algorithmus, der in der Version 9 im Jahr 2024 veröffentlicht wurde, ermöglicht eine schnelle und präzise Objekterkennung. \acrshort{YOLO} basiert auf einer einzigen neuronalen Netzwerkarchitektur, die das Bild in Raster unterteilt und für jedes Rasterfeld Vorhersagen über die Position und Klasse von Objekten trifft. 

Die Hauptvorteile von \acrshort{YOLO} sind seine Geschwindigkeit und Genauigkeit, die es ermöglichen, große Datensätze effizient zu analysieren. Für diese Arbeit wird \acrshort{YOLO} verwendet, um Fahrzeuge auf Luftbildern zu detektieren und zu klassifizieren. Dabei könnten zusätzliche multispektrale Kanäle wie \acrshort{NIR} oder \acrshort{IR} integriert werden, um einen Vergleich zu dem reinen 3 Kanal \acrshort{RGB} Training zu ermöglichen. Die Vergleichbarkeit kann gewährleistet werden, wenn die gleichen Trainings- und Testdaten verwendet werden, bei denen der einzige Unterschied das weitere Band ist.

\subsection{YOLOv9u}

Die Implementierung von \texttt{YOLOv9u} basiert auf der Version \texttt{YOLOv8.1.23}\cite{yolo_v9u_github} und stellt eine umfassende Weiterentwicklung der \acrshort{YOLO}-Architektur dar. Neben der klassischen Objekterkennung unterstützt \texttt{YOLOv9u} zusätzliche Aufgaben wie Instanzsegmentierung, orientierte Objekterkennung, Posenabschätzung, Bildklassifikation sowie transformerbasierte Objekterkennung\cite{wang2024}. Damit bietet das Modell eine erweiterte Funktionalität gegenüber früheren \acrshort{YOLO}-Versionen und eignet sich insbesondere für komplexe Szenarien mit mehreren Aufgabenstellungen innerhalb eines Frameworks.

\subsection{YOLOv9e}

Die \texttt{YOLOv9}-Modellreihe umfasst verschiedene Varianten, die sich hinsichtlich ihrer Modellgröße, Genauigkeit und Rechenkomplexität unterscheiden. Die Bandbreite reicht dabei von der kompakten \texttt{YOLOv9t}-Variante mit einer Eingabebildgröße von $640 \times 640$ Pixeln, 2 Millionen Parametern und 7{,}7 \Acrfullpl{GFLOP} bis hin zur leistungsstärksten \texttt{YOLOv9e}-Variante mit 58{,}1 Millionen Parametern und 192{,}5 \acrshortpl{GFLOP} (siehe Tabelle~\ref{tab:yolov9-models}).
\begin{table}[h]
\centering
\caption{Comparison of YOLOv9 model variants in terms of accuracy and computational complexity at an input resolution of 640$\times$640 pixels.}
\label{tab:yolov9-models}
\begin{tabular}{l|c|c|c|c|c} % keine äußeren vertikalen Linien
\textbf{Model} & \textbf{Image Size} & \textbf{mAP$_{\text{val}}$ 50--95} & \textbf{mAP$_{\text{val}}$ 50} & \textbf{Parameters (M)} & \textbf{FLOPs (B)} \\
\hline
YOLOv9t & 640 & 38.3 & 53.1 & 2.0 & 7.7 \\
\hline
YOLOv9s & 640 & 46.8 & 63.4 & 7.2 & 26.7 \\
\hline
YOLOv9m & 640 & 51.4 & 68.1 & 20.1 & 76.8 \\
\hline
YOLOv9c & 640 & 53.0 & 70.2 & 25.5 & 102.8 \\
\hline
YOLOv9e & 640 & 55.6 & 72.8 & 58.1 & 192.5 \\
\end{tabular}
\end{table}



Aufgrund der höchsten erzielten Genauigkeit (\acrshort{mAP}$_{\text{val}}^{50\text{-}95} = 55.6$) sowie der höchsten Rechenkapazität wurde das \texttt{YOLOv9e}-Modell für alle Experimente mit axis-aligned Bounding Boxes verwendet. Die Wahl dieses Modells erfolgte mit dem Ziel, die bestmögliche Detektionsleistung im Rahmen der Evaluierung zu gewährleisten.




% \subsection{YOLOv9u}
% \begin{itemize}
%     \item Implementierung von YOLOv9 auf Basis von YOLOv8.1.23 \cite{yolo_v9u_github}
%     \item erweiterung von yolov9 mit object detection, instance segmentation, oriented object detection, pose estimation, image classficitation, transformer-based object detection \cite{wang2024yolov9}
% \end{itemize}
% \subsection{YOLOv9e}
% \begin{itemize}
%     \item yolov9 Modelle reichen von der t variante mit (imgsize: 640, flops 7.7) bis zur e variante mit (192,5 gflops)
%     \item aufgrund der besten genauigketi und der höchsten flop rate wird das yolov9e modell bei den axis aligend bounding boxen genutzt
% \end{itemize}
\subsection{Hyperparameter}
\todo{wird bei Implementierung beschrieben, hier raus nehmen?}

\section{Database}
\subsection{Vehicle Detection on Aerial Images (VEDAI) Dataset}

Das \Acrshort{VEDAI} Dataset \cite{vedai_web}  aus dem Jahr 2015 bietet sich als Datengrundlage an, da es hochauflösende Luftbilder enthält, die speziell für die Fahrzeugerkennung geeignet sind \cite{Razakarivony2015}. Es umfasst annotierte Daten, die Fahrzeuge in unterschiedlichen Szenarien, Größen und Orientierungen zeigen. Außerdem ist es ein Benchmark für die Detektion von sehr kleinen Objekten. Das Dataset enthält sowohl \acrshort{RGB}-Bilder als auch multispektrale Daten, was es ideal für die Untersuchung der Auswirkungen zusätzlicher Kanäle wie \acrshort{NIR} oder \acrshort{IR} auf die Objekterkennung macht. Es sind mehr als 3700 Objekte in ungefähr 1200 Bildern annotiert. Diese Objekte sind in 9 Klassen (Boat, Camping Car, Car, Pickup, Plane, Tractor, Truck, Van und Others) unterteilt und der Hintergrund der Objekte ist abwechslungsreich, was die Robustheit des trainierten Modelles erhöht. \\
Die Auflösung der Bilder liegt bei 12.5 cm \texttimes 12.5 cm pro Pixel, was ausreichend ist um einzelne Fahrzeuge zu erkennen. Die Bilder mit einer Größe von 1024 \texttimes 1024 Pixeln sind bei einer Befliegung im Jahr 2012 in US Amerikanischen Bundesstaat Utah aufgenommen worden. 
\todo{Bsp. Bild für jede Klasse zeigen (aus Paper nehmen!; Quellenverlinkung für text und bild nicht vergessen)}



\subsection{Dataset for Objectdetection in Aerial Images (DOTA) 1.5}

Das \Acrfull{DOTA} eignet sich aufgrund seiner Vielfalt an Klassen und seiner domänenspezifischen Ausrichtung besonders gut als Grundlage für das Pretraining in den Permutationsexperimenten. Er enthält eine große Anzahl annotierter Objekte aus verschiedenen Kategorien, die in hochauflösenden Satellitenbildern enthalten sind.

Die Bildauflösungen im \acrshort{DOTA}-Datensatz variieren stark und reichen von 800~$\times$~800 bis hin zu 20\,000~$\times$~20\,000 Pixeln. Die Datensätze umfassen sowohl \acrshort{RGB}-Bilder als auch Graustufenbilder, die den panchromatischen Kanal der GF2- und JL1-Satelliten darstellen. Die Bilder stammen aus verschiedenen Quellen, unter anderem von Google Earth und weiteren kommerziellen oder institutionellen Anbietern.

Für das Training in dieser Arbeit wurden alle 16 verfügbaren Klassen des DOTA-Datensatzes berücksichtigt. Von besonderem Interesse für die anschließenden Experimente sind jedoch die Klassen \textit{Large Vehicle}, \textit{Small Vehicle}, \textit{Plane} und \textit{Ship}, da sie eine hohe Relevanz für die Objekterkennung im urbanen bzw. infrastrukturellen Kontext besitzen.

\todo{Hinweis: Für eine bessere Anschaulichkeit sollen beispielhafte Bilder für jede Klasse aus der Originalpublikation übernommen und entsprechend mit Quellenangabe referenziert werden.}

% \begin{itemize}
%     \item DOTA bietet sich als pretrained Model Datengrundlage für die Permutationsexperimente an, da es viele verschieden Klassen auf Satellitenbildern enthält
%     \item Bildgröße von 800 \texttimes 800 bis 20.000 \texttimes 20.000 Pixel
%     \item 3 Channel (Red, Green, Blue) und Grayscale Images (Panchromatic Band von GF2 und JL1 Satelliten)
%     \item Satellite Images von Google Earth und anderne Quellen
%     \item \todo{Bsp. Bild für jede Klasse zeigen (aus Paper nehmen!; Quellenverlinkung für text und bild nicht vergessen); habe alle 16 Klassen für das Training genutzt, wichtig sind trotzdem nur Large Vehicle, Plane, Ship, small vehicle}
% \end{itemize}


\section{Umsetzung}
\subsection{PALMA}

Für das Training der Modelle wurde das Hochleistungsrechenzentrum \Acrshort{PALMA} der Universität Münster genutzt. Als Arbeitsumgebung kam dabei eine isolierte \texttt{Python}-Umgebung (UV) zum Einsatz. Die Ausführung der Trainingsprozesse erfolgte über selbst entwickelte \texttt{Bash}-Skripte, die auf unterschiedlichen \acrshort{GPU}-Knoten des Clusters (u.\,a. HGX-Knoten) verteilt wurden, um eine effiziente Parallelisierung der Trainings zu ermöglichen.

PALMA wird von dem \Acrfull{CIT} der Universität Münster, basierend auf Hardware der Firma MEGWARE, bereitgestellt\cite{palma_spec} und verfügt über folgende technische Spezifikationen:

\begin{itemize}
    \item \textbf{Gesamtzahl der \acrshort{CPU}-Kerne:} 16\,272
    \item \textbf{Arbeitsspeicher:} 77\,568\,\acrshort{GB}
    \item \textbf{Anzahl der Rechenknoten:} 444
    \item \textbf{Prozessor:} Intel Xeon Gold 6140 (18~Kerne, 2{,}30\,\acrshort{GHz}, Skylake-Architektur)
    \item \textbf{Netzwerkinterconnect:} 100\,\acrshort{GHz}/s Intel Omni-Path
    \item \textbf{Speichersystem:} \acrshort{GPFS} mit einer Gesamtkapazität von 2{,}4\,\acrshort{PB}
    \item \textbf{Linpack-Leistung:} \textit{R\textsubscript{max}} = 800\,\acrshort{TFLOP}/s; \textit{R\textsubscript{peak}} = 1{,}277\,\acrshort{TFLOP}/s
    \item \textbf{Betriebssystem:} Rocky Linux 9
\end{itemize}

Die hohe Rechenleistung und Speicherkapazität des Clusters machten es möglich, auch komplexe Trainingsprozesse mit großen Bilddaten effizient zu verarbeiten.
% \begin{itemize}
%     \item UV als Python Umgebung auf Palma genutzt
%     \item Bash Scripte geschrieben um die Modelle auf diversen GPUS (HGX, etc) zu trainieren
% \end{itemize}
% \begin{itemize}
%     \item Hersteller: MEGWARE\cite{palma_spec}
%     \item 16.272 Cores
%     \item 77.568 GB Memory
%     \item 444 Nodes
%     \item Processor: Intel Xeon Gold 6140 18C @ 2.30GHz (Skylake)
%     \item Interconnect 100Gbit/s Intel Omni-Path
%     \item GPFS Storage: 2,4 PB
%     \item Linpack Performance: Rmax: 800 TFlop/sRpeak: 1,277 TFlop/s
%     \item OS: Rocky Linux 9 
% \end{itemize}

% \subsection{Challenges Preprocessing}
% \subsubsection{VEDAI Dataset Challenges}
% \begin{itemize}
%     \item Label müssen in yolov9 obb format konvertiert werden
%     \item kleine Anzahl (7/3757) war kleiner als 0 oder größer als 1
%     \item Lösung mit Exception Handling und Runden der Werte
%     \item \todo{Beispielbilder einfügen}
% \end{itemize}

% \section{Workflow}
% \subsection{Axis Aligned vs. Oriented Bounding Boxes}
% \begin{itemize}
%     \item Vergleich zwischen Axis Aligned udn Oriented Bounding Boxen
%     \item YOLOv9 arbeitet ursprünglich nur mit aab Boxen
%     \item YOLOv9u kann mit obb arbeiten, da Codebasis von YOLOv8 von Ultralytics, was obb unterstützt
%     \item 
% \end{itemize}
% \begin{itemize}
%     \item \todo{Vergleichsbilder (Schiff und Auto Vergleich) einfügen}
%     \item mehr Blankspace bei axis aligned Bbs
%     \item Concentration of the box on the actual object, significantly less surrounding area outlined. No overlap between bounding boxes (Bei Ship bb)
%     \item 
% \end{itemize}
% \subsection{Channel Permutation}
% \begin{itemize}
%     \item Folgende Permutation werden im Rahmen der Arbeit evaluiert: RGBIR, IRGB, RIRB; RGB, RGIR, RGBNDVI, GBNDBVI
% \end{itemize}
% \subsection{DOTA Training}
% \begin{itemize}
%     \item DOTA als Pretrained Modell für Channel Permutation, map50-95 around 0.4 and training over around 800 epcohs 
%     \item \todo{Result.png einfügen? oder doch sein lassen?}
% \end{itemize}
% \subsection{Ablation Studie}
% \begin{itemize}
%     \item Ablation Study für R, G, B, IR, NDVI durchgeführt
% \end{itemize}


